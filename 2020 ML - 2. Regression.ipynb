{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lecture 1: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](regression_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n",
    "\n",
    "这是一个典型的regression problem.我们的input是某一只宝可梦所有相关的information. 比如说，我们把一只宝可梦用X表示，\n",
    "* 它的CP值用$X_{cp}$来表示，代表在它进化以前的CP值。\n",
    "* $X_s$代表这一只宝可梦X，是属于哪一个物种。\n",
    "* $X_hp$代表这一只宝可梦，它的hp值是多少，它的生命值是多少。\n",
    "* $X_w$代表它的重量。\n",
    "* $X_h$代表它的高度。\n",
    "\n",
    "* Output是进化后的CP值\n",
    "\n",
    "![](regression_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Steps\n",
    "\n",
    "1. step 1: 第一个步骤就是要找到一个model (function set).\n",
    "2. step 2: 第二个步骤就是定义function set里面某一个function,我们可以拿一个function出来evaluate它的好坏。\n",
    "3. step 3: 第三个步骤就是找一个最好的function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Model\n",
    "\n",
    "* $X_{cp}$代表进化前的CP值，y代表进化后的CP值\n",
    "* $w$和$b$是参数，可以是任何数值\n",
    "* 在这个model里面，w和b是未知的。你可以填进去不同的数值，你就可以得到不同的function. \n",
    "\n",
    "\n",
    "我们要用training data来告诉我们，在function set里面，哪一个function才是合理的function. \n",
    "\n",
    "![](regression_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2：Goodness of Function\n",
    "\n",
    "\n",
    "## Part 1: \n",
    "* 接下来，我们要收集traning data,才能够找这个function.\n",
    "* 这是一个supervised learning task,所以我们收集的是function的input,和function的output.\n",
    "* 因为是regression的task,所以function的output是一个scalar. \n",
    "* 用下标表示component, 用上标来表示一个完整object的编号。\n",
    "\n",
    "![](regression_4.png)\n",
    "\n",
    "\n",
    "## Part 2: \n",
    "![](regression_5.png)\n",
    "\n",
    "\n",
    "## Part 3: \n",
    "\n",
    "* 判定一个function好不好，我们需要定义另外一个function: loss function\n",
    "* Loss function是一个很特别的function. 这个loss function是function的function。\n",
    "* Loss function的input是一个function,它的output是how bad it is. \n",
    "* $L(f) = L(w, b)$\n",
    "* Input就是一个function $f$, 你知道一个function它是由这个function里面的两个参数$b$和$w$来决定的，即$f$是由$b$和$w$来决定的。\n",
    "* 所以你可以说，loss function它是在衡量一组参数的好坏 （衡量一组$b$和$w$的好坏）\n",
    "* loss function的选择其实可以随自己的喜好。定义一个自己觉得合理的function. \n",
    "* 常见做法是，把input $w$和$b$,实际代入$y = b + w*X_{cp}$这个function里面，就得到estimated y based on input function. \n",
    "* $\\hat y$是真正的数值，我们用$\\hat y$减去estimated y, 再取平方，这个就是估测的误差 - estimated error. \n",
    "* 我们把手上的10只宝可梦都合起来，就得到这个Loss Function. \n",
    "* 我们就用估测误差来定义一个Loss function.估测误差越大，这个function就越不好。\n",
    "\n",
    "![](regression_6.png)\n",
    "\n",
    "## Part 4: \n",
    "\n",
    "* 我们有了loss function以后，我们可以把loss function的形状画出来。\n",
    "* 图上的颜色代表了，现在如果我们使用这个function,根据我们定义的loss function,它有多糟糕。图上的颜色越偏红色，就代表数值越大。越偏蓝色，说明那个function越好。\n",
    "\n",
    "![](regression_7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3：Best Function\n",
    "\n",
    "* 我们已经定义好了一个Loss function,现在我们要左的就是从function set里面，挑出一个最好的function.\n",
    "* 挑选最好的function这一件事情，如果想要把它写成formulation的话，如下 - \n",
    "* $f^{*} = arg min L(f)$\n",
    "* or 我们知道f是由两个参数$w$和$b$表示，我们就穷举所有的$w$和$b$,看哪一个$w$和$b$代入可以让loss的值最小。那么这个$w$和$b$就是最好的。\n",
    "* $ w^{*}, b^{*} = arg min L(w, b)$\n",
    "* Gradient descent不是只适用于解这一种function.\n",
    "\n",
    "## Part 1: \n",
    "\n",
    "\n",
    "![](regression_8.png)\n",
    "\n",
    "\n",
    "\n",
    "## Part 2: Gradient Descent\n",
    "\n",
    "\n",
    "![](regression_9.png)\n",
    "\n",
    "\n",
    "## Part 3: Gradient Descent\n",
    "\n",
    "* 我们先假设一个比较简单的task,在这个task里面，我们的loss function L(w),它只有一个参数w. \n",
    "* Step1: 先随机选择一个$w^0$\n",
    "* Step2: 在这个初始的$w^0$的位置，我们计算一下w这个参数对Loss Function的微分。\n",
    "* 如果切线斜率是负的话，显然左边loss比较高，右边loss比较低，那我们要找loss比较低的function，所有应该增加$w$的值。\n",
    "* 如果切线斜率是正的话，显示是右边的loss比较高，左边的斜率比较低，那我们就要减小$w$的值，把我们的参数往左边移动。\n",
    "\n",
    "* 如果我们需要右移，那么要移动多少呢？踏一步的step size,取决于两件事情 - \n",
    " 1. 现在的微分值有多大，即 $frac{dL}{dW}$有多大。如果微分值越大，代表现在在一个越陡峭的地方，那它移动的距离就越大，反之就越小。\n",
    " 2. 另一个是一个常数项，$\\eta$ is called \"learning rate\". Learning Rate决定，我们今天踏一步，不只是取决于我们现在的微分值算出来有多大，还取决于我们一个事先就定好的数值。如果事先把learning rate定的大一点，那今天踏出一步的时候，参数更新的幅度就比较大，反之，参数更新的幅度就比较小。如果learning rate比较大，那么学习的效率就高。\n",
    "\n",
    "\n",
    "![](regression_10.png)\n",
    "\n",
    "\n",
    "## Part 4: Gradient Descent\n",
    "\n",
    "* 我们把$w^0$更新以后，变成了$w^1$.\n",
    "* 接下来就是重复刚才的步骤。重新计算一下，再$w = w^1$这个位置，所算出来的微分值。\n",
    "* 这个步骤反复不断的执行下去。经过非常多次的参数更新以后，我们会到达一个local minimum的地方。即微分是0。我们的参数会卡在这里，不再更新。\n",
    "* 但是停留在local minimum而无法找到global optimal对于linear regression来说，不是一个问题。因为，linear regression没有local optimal.\n",
    "\n",
    "\n",
    "![](regression_11.png)\n",
    "\n",
    "\n",
    "## Part 5: Gradient Descent - two parameters\n",
    "\n",
    "* 如果有两个参数，$w$和$b$.\n",
    "* 从一个参数，推广到两个参数，其实是没有任何不同的。\n",
    "* step1: 随机选取两个初始值，$w^0$, $b^0$\n",
    "* step2: 在$w = w^0$, $b = b^0$的时候，计算$w$对Loss的偏微分，同时$w = w^0$, $b = b^0$的时候，计算$b$对Loss的偏微分\n",
    "* 计算出这两个偏微分以后，就分别去更新$w^0$, $b^0$这两个参数。\n",
    "* 反复持续这个步骤。\n",
    "\n",
    "* Gradient descent中的Gradient指的是什么？其实gradient就是这个倒三角L。就是把$w$对L的偏微分和$b$对L的偏微分排成一个vector,这一项就是gradient. \n",
    "\n",
    "\n",
    "![](regression_12.png)\n",
    "\n",
    "\n",
    "## Part 6: Gradient Descent - Visualize\n",
    "\n",
    "![](regression_13.png)\n",
    "\n",
    "## Part 7: Gradient Descent - Worry?\n",
    "\n",
    "* 如果loss function是下图的样子，那就存在local optimal和global optimal。\n",
    "* 取决于$w$, $b$的初始值取在哪里，很可能只能到达local optimal.\n",
    "* 但是，这个情况不存在于linear regression里面，因为linear regression的loss function L is convex.\n",
    "* Convex也即没有local optimal. 所以随便选一个初始点，最后找出来的都是同一组参数。\n",
    "\n",
    "\n",
    "## Part 7: Gradient Descent - Formualtion \n",
    "\n",
    "![](regression_14.png)\n",
    "![](regression_16.png)\n",
    "\n",
    "\n",
    "\n",
    "## Part 8: How is the Results?\n",
    "\n",
    "\n",
    "![](regression_18.png)\n",
    "\n",
    "\n",
    "## Part 9: How is the Results? - Generalization\n",
    "\n",
    "* 但是，我们真正关心的是，generalization的case,就是有一个新的数据，用这个model去估测，那么做出来你估测的误差到底有多少？\n",
    "* We take another 10 pokemons as testing data.和之前做训练的10只是不同的10只。\n",
    "* Average error on testing data = 35.0, 比我们在training data上的error大一点\n",
    "* How can we do better? -- 重新设计model - 我们可能需要一个更加复杂的model\n",
    "\n",
    "![](regression_19.png)\n",
    "\n",
    "## Part 10: Selecting another model\n",
    "\n",
    "* 考虑平方项\n",
    "* 可以更好吗？ -- 考虑立方项\n",
    "\n",
    "![](regression_20.png)\n",
    "\n",
    "## Part 11: Selecting another model\n",
    "\n",
    "* 是否可能是更加复杂的model呢？ -- 有可能。\n",
    "\n",
    "![](regression_21.png)\n",
    "\n",
    "## Part 12: Selecting another model\n",
    "\n",
    "* Training data error降低到了14.9.\n",
    "* 但是，在我们真正关心的testing data上的error居然到了28.8. The results get worse.\n",
    "\n",
    "![](regression_22.png)\n",
    "\n",
    "## Part 13: Selecting another model\n",
    "\n",
    "* Training data error降低到了12.8.\n",
    "* 但是，在我们真正关心的testing data上的error居然到了232.1. Very bad performance.\n",
    "\n",
    "![](regression_23.png)\n",
    "\n",
    "## Part 14: Model Selection\n",
    "\n",
    "![](regression_24.png)\n",
    "\n",
    "\n",
    "## Part 15: Model Selection\n",
    "\n",
    "* 在testing data上，不一定function越复杂，performance越好。\n",
    "* A more complex model does not always lead to better performance on testing data. This is __overfitting__.\n",
    "* 所以，model不是越复杂越好。我们必须选一个刚刚好，没有非常复杂的model,你需要选一个适合的model.\n",
    "\n",
    "![](regression_25.png)\n",
    "\n",
    "\n",
    "## Part 16: Let's collect more data\n",
    "\n",
    "![](regression_26.png)\n",
    "\n",
    "\n",
    "## Part 17: What are the hidden factors?\n",
    "\n",
    "* 只考虑进化前的CP值明显是不对的。因为进化后的CP值受到物种的影响是非常大的。所以，我们设计model的时候，要考虑到物种。\n",
    "\n",
    "![](regression_27.png)\n",
    "\n",
    "## Part 18: Back to step 1: Redesign the Model\n",
    "\n",
    "* 我们要看是哪一个物种。不同的物种，我们就代入不同的linear function. 然后得到不同的y作为最终的输出。\n",
    "\n",
    "![](regression_28.png)\n",
    "![](regression_29.png)\n",
    "![](regression_30.png)\n",
    "![](regression_31.png)\n",
    "![](regression_32.png)\n",
    "\n",
    "\n",
    "## Part 19: Back to step 2: Regularization\n",
    "\n",
    "* 因为overfitting,我们返回到step 2,重新定义一个function是好的还是坏的定义。我们重新定义loss function. 把一些knowledge放进去，让我们可以找到比较好的function.\n",
    "* 在原来的function上，加入另外一个term.\n",
    "* 加入这个term,因为我们期待L非常小，就意味这我们期待这个新加入的term也很小，比如说趋近于0，那么也就是参数越小越好.但是，为什么，我们想要一个参数趋近于0的function呢？因为参数值接近0的function,它是比较平滑的。所谓平滑的意思是，当今天的输入有变化的时候，output对输入的变化是比较不敏感的。\n",
    "* 为什么参数小就会达到这个效果呢？因为input都是乘上参数的。所以参数越小，那么output对input就越不敏感。\n",
    "* 为什么我们更加喜欢平滑的function呢？因为平滑的function,对输入的变化不敏感。如果我们的输入被杂讯所干扰的话，那一个比较平滑的function,它受到的影响就比较少，而给我们比较好的结果。\n",
    "\n",
    "![](regression_33.png)\n",
    "\n",
    "\n",
    "## Part 20: Back to step 2: Regularization - Effects\n",
    "\n",
    "* $\\lambda$越大，我们找到的function越平滑。因为$\\lambda$越大，我们越倾向于考虑W本来的值，而减少考虑我们的error.所以，$\\lambda$越大,我们考虑error就越少。所以我们在traning data上得到的error就越大。\n",
    "* 但是，虽然我们training data上得到的error虽然比较大，但是，在testing data上得到error可能是会比较小。\n",
    "* 我们可以看到，$\\lambda$等于0， error比较大，$\\lambda$等于1的时候，开始降低，$\\lambda$等于100的时候，两个error都比较小，但是到$\\lambda$到1000的时候，error又开始变大了。\n",
    "* 这是非常合理的。我们喜欢比较平滑的function,比较平滑的function对noise不是那么的敏感。所以，当我们增加$\\lambda$的时候，function的performance是越来越好的。但是，我们又不喜欢太平滑的function,如果你的function是一条水平线，那就不work了。\n",
    "\n",
    "![](regression_34.png)\n",
    "\n",
    "\n",
    "## Part 21: Back to step 2: Regularization - How smooth?\n",
    "\n",
    "* 通过调整$\\lambda$来调整function的平滑程度。\n",
    "* 在Regularization上里面，没有加入b,是正确的。因为我们的预期是找到一个比较平滑的function,但是bias跟一个function的平滑程度是没有关系的。调整bias的值大小时，只是把function上下移动而已。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Part 22: Conclusion & Following Lectures\n",
    "![](regression_35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
